package tcd;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.analysis.core.SimpleAnalyzer;
import org.apache.lucene.analysis.core.StopAnalyzer;
import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
import org.apache.lucene.analysis.en.EnglishAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.queryparser.classic.MultiFieldQueryParser;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.similarities.BM25Similarity;
import org.apache.lucene.search.similarities.ClassicSimilarity;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;

import java.io.File;
import java.io.IOException;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * Implements search functionality from index created using Indexer wrapper.
 *
 */
public class Searcher {

	private int HITS_PER_PAGE = 1000;
		
	/**
	 * search cran queries in index generated by Indexer.
	 * @param indexFile: location of the index file
	 * @param cranQueryList: list of cran queries`
	 * @param analyzerString: lucene analyzer to use
	 * @param similarity: lucene similarity to use
	 * @param hpp: hits per page to use
	 * @return ranked list of documents for each queries
	 * @throws ParseException
	 */
	public Map<String, List<String>> searchCranQueries(Path indexFile, List<Map<String, String>> cranQueryList, String analyzerString, String similarity, String hpp) throws ParseException {
		
		Map<String, List<String>> resultDict = new HashMap<String, List<String>>();
		try {
			
			try {
				HITS_PER_PAGE = Integer.parseInt(hpp);
			}
			catch (NumberFormatException e) {}
			
			// Create the same analyzer as the indexer
			Analyzer analyzer = null;
			if (analyzerString.equals("Standard")) analyzer = new StandardAnalyzer(EnglishAnalyzer.getDefaultStopSet());
			else if (analyzerString.equals("Keyword")) analyzer = new KeywordAnalyzer();
			else if (analyzerString.equals("WhiteSpace")) analyzer = new WhitespaceAnalyzer();
			else if (analyzerString.equals("Simple")) analyzer = new SimpleAnalyzer();
			else if (analyzerString.equals("Stop")) analyzer = new StopAnalyzer();
			else {
				
				analyzer = new EnglishAnalyzer();
				analyzerString = "English";
			}
			
			// Get index from disk
			Directory directory = FSDirectory.open(indexFile);
			DirectoryReader ireader = DirectoryReader.open(directory);
			
			// Create an index searcher
			IndexSearcher isearcher = new IndexSearcher(ireader);
			if (similarity.equals("TFIDF")) isearcher.setSimilarity(new ClassicSimilarity());
			else if (similarity.equals("BM25")) isearcher.setSimilarity(new BM25Similarity());
		//	else if (similarity.equals("LMDirichlet")) isearcher.setSimilarity(new LMDirichletSimilarity());

						
			List<String> resFileContent = new ArrayList<String>();
			System.out.println("Searching index using " + analyzerString + " analyzer and " + similarity + " similarity, with " + Integer.toString(HITS_PER_PAGE) + " hits per page.");
			System.out.println("Please wait for some seconds :) ....in progress .........");
			// Loop through all queries and retrieve documents
			for (int i = 0; i < cranQueryList.size(); i++) {
				
				Map<String, String> cranQuery = cranQueryList.get(i);
				MultiFieldQueryParser queryParser = new MultiFieldQueryParser(
						new String[] {"Title", "Locations", "Authors", "Abstract"},
						analyzer);
				Query query = queryParser.parse(cranQuery.get("Query"));
				
				// Search
				TopDocs topDocs = isearcher.search(query, HITS_PER_PAGE);
				ScoreDoc[] hits = topDocs.scoreDocs;
				
				// Display results
				List<String> resultList = new ArrayList<String>();
//		        System.out.println("Found " + hits.length + " hits.");
		        for(int j = 0; j < hits.length; j++) {
		            
		        	int docId = hits[j].doc;
		            Document doc = isearcher.doc(docId);
		            resultList.add(doc.get("ID"));
		            resFileContent.add(cranQuery.get("QueryNo") + " 0 " + doc.get("ID") + " 0 " + hits[j].score + " STANDARD\n");
		        }
		        resultDict.put(Integer.toString(i + 1), resultList);
			}
			
			// Create directory if it does not exist
			File outputDir = new File("output");
			if (!outputDir.exists()) outputDir.mkdir();
			
			Files.write(Paths.get("output/results.txt"), resFileContent, Charset.forName("UTF-8"));
			System.out.println("Results written to output/results.txt to be used in TREC Eval.");
		}
		catch (IOException e) {
			
			e.printStackTrace();
			System.exit(1);
		}
		return resultDict;
	}
	
	/**
	 * Main method
	 * @param args: Command line arguments
	 * @throws ParseException
	 */

	public static void main(String[] args) throws ParseException {

		String analyzer = "English";
		String similarity = "BM25";
		String hpp = "1000";
		String dataDir="data/cran";
		System.out.println("Parsing CRAN Queries...");
		FileIO fileIO = new FileIO();
		List<Map<String, String>> cranQueryList = fileIO.parseCranQueries("data/cran");
		System.out.println("Parsing done!\n");
		
		System.out.println("Searching data...");
		Searcher searcher = new Searcher();
		searcher.searchCranQueries(Paths.get("index/cran.index"), cranQueryList, analyzer, similarity, hpp);
		System.out.println("Searching done!");
	}


}
